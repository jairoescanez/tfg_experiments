{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Método de la M grande, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.5484],\n",
       "        [ 0.5968],\n",
       "        ...,\n",
       "        [49.9032],\n",
       "        [49.9516],\n",
       "        [50.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(0.5, 50, 1024).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w + u**2 / 2)],\n",
    "             [torch.sin(w + u**2 / 2)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],      # x1 >= 0\n",
    "                [ 0.0, -1.0],      # x2 >= 0\n",
    "                [ 1.0,  0.0],      # x1 <= 2*w\n",
    "                [ .5*w, w]]  # (1+w)*x1 + 2*(1+w)*x2 <= u\n",
    "\n",
    "        b_ub = [[ 0.0],\n",
    "                [ 0.0],\n",
    "                [ 4/u],\n",
    "                [   u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[0.8])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "# x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "# torch.save(x_train, \"x_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3378e-06, 4.7761e-06],\n",
       "        [6.4988e-06, 4.6438e-06],\n",
       "        [6.1619e-06, 4.1501e-06],\n",
       "        ...,\n",
       "        [8.0112e-02, 1.4527e-05],\n",
       "        [1.9709e-04, 6.2439e+01],\n",
       "        [2.5363e-05, 5.3045e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "# u_data = [[1.0], [2.0], [3.0], [4.0]]\n",
    "# x_targets = [[1.0, 1.5], [2.0, 2.5], [3.0, 3.5], [4.0, 4.5]]\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qué)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return c, A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe preservar el grafo de computo para poder hacer backpropagation\n",
    "def smooth_lp(c, A, b):\n",
    "    # Inicializar x con gradientes habilitados\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD([x], lr=learning_rate)\n",
    "\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        objective = torch.dot(c, x) + 1000.0 * constraint_penalty\n",
    "        objective.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de perdida\n",
    "def my_loss(c, A, b, target):\n",
    "    rs = smooth_lp(c, A, b)\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn(c, A, b, target):\n",
    "    return torch.mean(torch.stack([my_loss(c[i], A[i], b[i], target[i]) for i in range(len(c))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # elegir una de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "\n",
    "        c, A, b = model(u_batch)\n",
    "        # Calcular la pérdida\n",
    "        #loss = my_loss(c[0], A[0], b[0], x_batch[0])  # Usar el target correspondiente\n",
    "        loss = loss_fn(c, A, b, x_batch)\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss, current = loss.item(), batch * batch_size + len(u_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        c, A, b = model(u_batch)\n",
    "        test_loss += loss_fn(c, A, b, x_batch).item()\n",
    "        x_pred = torch.stack([smooth_lp(c[i], A[i], b[i]) for i in range(len(c))])\n",
    "        correct += torch.sum(x_pred == x_batch).item() # cambiar esto, aqui poner la solucion del problema\n",
    "        # y consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 751.658081  [    8/ 1024]\n",
      "loss: 406.782806  [   72/ 1024]\n",
      "loss: 323.025909  [  136/ 1024]\n",
      "loss: 706.061523  [  200/ 1024]\n",
      "loss: 270.391846  [  264/ 1024]\n",
      "loss: 1138.208984  [  328/ 1024]\n",
      "loss: 972.936401  [  392/ 1024]\n",
      "loss: 536.850708  [  456/ 1024]\n",
      "loss: 161.913574  [  520/ 1024]\n",
      "loss: 630.460999  [  584/ 1024]\n",
      "loss: 448.160828  [  648/ 1024]\n",
      "loss: 878.794006  [  712/ 1024]\n",
      "loss: 392.461365  [  776/ 1024]\n",
      "loss: 636.737366  [  840/ 1024]\n",
      "loss: 305.131714  [  904/ 1024]\n",
      "loss: 430.907837  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 639.000893 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 910.739624  [    8/ 1024]\n",
      "loss: 598.135925  [   72/ 1024]\n",
      "loss: 673.777466  [  136/ 1024]\n",
      "loss: 1124.274902  [  200/ 1024]\n",
      "loss: 793.530579  [  264/ 1024]\n",
      "loss: 814.863525  [  328/ 1024]\n",
      "loss: 74.730316  [  392/ 1024]\n",
      "loss: 615.075745  [  456/ 1024]\n",
      "loss: 531.544312  [  520/ 1024]\n",
      "loss: 398.201904  [  584/ 1024]\n",
      "loss: 197.492126  [  648/ 1024]\n",
      "loss: 127.287521  [  712/ 1024]\n",
      "loss: 579.338562  [  776/ 1024]\n",
      "loss: 726.397400  [  840/ 1024]\n",
      "loss: 1225.153198  [  904/ 1024]\n",
      "loss: 373.116455  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 639.000897 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1125.084351  [    8/ 1024]\n",
      "loss: 325.764954  [   72/ 1024]\n",
      "loss: 669.022339  [  136/ 1024]\n",
      "loss: 67.274315  [  200/ 1024]\n",
      "loss: 621.094360  [  264/ 1024]\n",
      "loss: 656.357788  [  328/ 1024]\n",
      "loss: 119.747337  [  392/ 1024]\n",
      "loss: 376.442719  [  456/ 1024]\n",
      "loss: 81.092003  [  520/ 1024]\n",
      "loss: 842.756714  [  584/ 1024]\n",
      "loss: 19.782906  [  648/ 1024]\n",
      "loss: 1161.933838  [  712/ 1024]\n",
      "loss: 478.029297  [  776/ 1024]\n",
      "loss: 1083.352661  [  840/ 1024]\n",
      "loss: 366.937378  [  904/ 1024]\n",
      "loss: 1054.134766  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 639.000889 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1111.422852  [    8/ 1024]\n",
      "loss: 519.805725  [   72/ 1024]\n",
      "loss: 647.872131  [  136/ 1024]\n",
      "loss: 432.931702  [  200/ 1024]\n",
      "loss: 316.999969  [  264/ 1024]\n",
      "loss: 137.112289  [  328/ 1024]\n",
      "loss: 497.010559  [  392/ 1024]\n",
      "loss: 514.430054  [  456/ 1024]\n",
      "loss: 834.686157  [  520/ 1024]\n",
      "loss: 847.879700  [  584/ 1024]\n",
      "loss: 307.159149  [  648/ 1024]\n",
      "loss: 456.469971  [  712/ 1024]\n",
      "loss: 416.823792  [  776/ 1024]\n",
      "loss: 645.336853  [  840/ 1024]\n",
      "loss: 1079.212646  [  904/ 1024]\n",
      "loss: 374.324158  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 639.000886 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 4 # poner mas\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1192, -0.0461], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c,A,b=model(torch.tensor([[1.0]]))\n",
    "smooth_lp(c[0], A[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `validacion` not found.\n"
     ]
    }
   ],
   "source": [
    "en el dataloader hay alguna manera de añadirle un conjunto de validacion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5031e-06, 5.2914e-06],\n",
       "        [5.8791e-06, 5.0125e-06],\n",
       "        [6.2946e-06, 4.1114e-06],\n",
       "        [1.5288e-05, 6.1703e-06],\n",
       "        [6.3201e-05, 7.6848e-06],\n",
       "        [2.8571e+00, 7.7249e-06],\n",
       "        [2.4096e+00, 8.3817e-06],\n",
       "        [2.0833e+00, 1.4402e-05],\n",
       "        [1.8349e+00, 1.8074e+00],\n",
       "        [1.6393e+00, 2.2303e+00],\n",
       "        [6.9978e-05, 3.3749e+00],\n",
       "        [1.2749e-05, 3.7000e+00],\n",
       "        [6.8515e-06, 4.0250e+00],\n",
       "        [1.1909e-05, 1.8498e-05],\n",
       "        [2.2888e-04, 1.3794e-05],\n",
       "        [9.9999e-01, 1.9565e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1622, -0.2429]], grad_fn=<SliceBackward0>),\n",
       " tensor([[[-0.4103,  0.0860],\n",
       "          [ 0.0938,  0.0168]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.1193, 0.0358]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, A, b = model(torch.tensor([[0.25]]))\n",
    "x_pred = smooth_lp(c[0], A[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64),\n",
       " tensor([-0.0162,  0.0243], requires_grad=True),\n",
       " tensor(0.0292, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se parecen bastante, ahora el siguiente paso es hacer la validacion, ya lo dejo para mañana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
