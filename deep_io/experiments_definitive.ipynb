{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es implementar una red neuronal que tenga como entrada el vector $u_i$ y como salida: $c$, $A$ y $b$. Que son los parametros del modelo de optimizacion. La funcion de perdida para entrenar la red neuronal sera la siguiente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left\\| x_i - \\hat{x}_i \\right\\|_2^2\n",
    "$$\n",
    "donde $\\hat{x}_i$ es la solucion del modelo de optimizacion con parametros $c$, $A$ y $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000222B2D4B060>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.296704  [   64/60000]\n",
      "loss: 2.286694  [ 6464/60000]\n",
      "loss: 2.268586  [12864/60000]\n",
      "loss: 2.268380  [19264/60000]\n",
      "loss: 2.255245  [25664/60000]\n",
      "loss: 2.215268  [32064/60000]\n",
      "loss: 2.235418  [38464/60000]\n",
      "loss: 2.189306  [44864/60000]\n",
      "loss: 2.192062  [51264/60000]\n",
      "loss: 2.164429  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.3%, Avg loss: 2.156784 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.165854  [   64/60000]\n",
      "loss: 2.156527  [ 6464/60000]\n",
      "loss: 2.095820  [12864/60000]\n",
      "loss: 2.117990  [19264/60000]\n",
      "loss: 2.084201  [25664/60000]\n",
      "loss: 2.010549  [32064/60000]\n",
      "loss: 2.048114  [38464/60000]\n",
      "loss: 1.958934  [44864/60000]\n",
      "loss: 1.962681  [51264/60000]\n",
      "loss: 1.902870  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Avg loss: 1.893457 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.920831  [   64/60000]\n",
      "loss: 1.897100  [ 6464/60000]\n",
      "loss: 1.772069  [12864/60000]\n",
      "loss: 1.818732  [19264/60000]\n",
      "loss: 1.739486  [25664/60000]\n",
      "loss: 1.665547  [32064/60000]\n",
      "loss: 1.695349  [38464/60000]\n",
      "loss: 1.583128  [44864/60000]\n",
      "loss: 1.610869  [51264/60000]\n",
      "loss: 1.509570  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Avg loss: 1.523711 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.583417  [   64/60000]\n",
      "loss: 1.560165  [ 6464/60000]\n",
      "loss: 1.399476  [12864/60000]\n",
      "loss: 1.477025  [19264/60000]\n",
      "loss: 1.379067  [25664/60000]\n",
      "loss: 1.349356  [32064/60000]\n",
      "loss: 1.372547  [38464/60000]\n",
      "loss: 1.283090  [44864/60000]\n",
      "loss: 1.328457  [51264/60000]\n",
      "loss: 1.226484  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.252560 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.321095  [   64/60000]\n",
      "loss: 1.317551  [ 6464/60000]\n",
      "loss: 1.142650  [12864/60000]\n",
      "loss: 1.252538  [19264/60000]\n",
      "loss: 1.138375  [25664/60000]\n",
      "loss: 1.148436  [32064/60000]\n",
      "loss: 1.176508  [38464/60000]\n",
      "loss: 1.100918  [44864/60000]\n",
      "loss: 1.151585  [51264/60000]\n",
      "loss: 1.065765  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 1.086334 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.147823  [   64/60000]\n",
      "loss: 1.165493  [ 6464/60000]\n",
      "loss: 0.975474  [12864/60000]\n",
      "loss: 1.112489  [19264/60000]\n",
      "loss: 0.994273  [25664/60000]\n",
      "loss: 1.016672  [32064/60000]\n",
      "loss: 1.058079  [38464/60000]\n",
      "loss: 0.986822  [44864/60000]\n",
      "loss: 1.037856  [51264/60000]\n",
      "loss: 0.966482  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 0.980198 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.029447  [   64/60000]\n",
      "loss: 1.067457  [ 6464/60000]\n",
      "loss: 0.861928  [12864/60000]\n",
      "loss: 1.019144  [19264/60000]\n",
      "loss: 0.905831  [25664/60000]\n",
      "loss: 0.924299  [32064/60000]\n",
      "loss: 0.980957  [38464/60000]\n",
      "loss: 0.912883  [44864/60000]\n",
      "loss: 0.959350  [51264/60000]\n",
      "loss: 0.899822  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.3%, Avg loss: 0.907717 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.942365  [   64/60000]\n",
      "loss: 0.999005  [ 6464/60000]\n",
      "loss: 0.780266  [12864/60000]\n",
      "loss: 0.952463  [19264/60000]\n",
      "loss: 0.847367  [25664/60000]\n",
      "loss: 0.856436  [32064/60000]\n",
      "loss: 0.926114  [38464/60000]\n",
      "loss: 0.862947  [44864/60000]\n",
      "loss: 0.902598  [51264/60000]\n",
      "loss: 0.851082  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.3%, Avg loss: 0.855198 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.875123  [   64/60000]\n",
      "loss: 0.947121  [ 6464/60000]\n",
      "loss: 0.718907  [12864/60000]\n",
      "loss: 0.902659  [19264/60000]\n",
      "loss: 0.805884  [25664/60000]\n",
      "loss: 0.805365  [32064/60000]\n",
      "loss: 0.884291  [38464/60000]\n",
      "loss: 0.828176  [44864/60000]\n",
      "loss: 0.860241  [51264/60000]\n",
      "loss: 0.813339  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.815292 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.821627  [   64/60000]\n",
      "loss: 0.905453  [ 6464/60000]\n",
      "loss: 0.671114  [12864/60000]\n",
      "loss: 0.864111  [19264/60000]\n",
      "loss: 0.774557  [25664/60000]\n",
      "loss: 0.766023  [32064/60000]\n",
      "loss: 0.850303  [38464/60000]\n",
      "loss: 0.802472  [44864/60000]\n",
      "loss: 0.827448  [51264/60000]\n",
      "loss: 0.782949  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.9%, Avg loss: 0.783537 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "funcion loss personalizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0005, 0.0009],\n",
      "        [0.0310, 0.0484]])\n"
     ]
    }
   ],
   "source": [
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "loss = my_loss(output, target)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import deep_inv_opt.linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora mi funcion de perdida adaptada a mi modelo\n",
    "def my_loss(output, target):\n",
    "    # output es la salida de la red neuronal, que me devuelve la matriz A y el vector b\n",
    "    # y el vector c de coste\n",
    "    c = output[0:2]\n",
    "    A = output[2:6].reshape(2, 2)\n",
    "    b = output[6:8]\n",
    "    rs = solver.linprog(c, A, b)\n",
    "    loss = torch.sum((rs-target)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voy a probar si funciona bien linprog\n",
    "c = torch.tensor([1.0, -1.0])\n",
    "A = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
    "b = torch.tensor([[1.0], [1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdeep_inv_opt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinprog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\jairoescanez\\onedrive - universidad de sevilla\\uni\\5º\\tfg\\deep_inv_opt\\deep_inv_opt\\linprog.py:250\u001b[0m, in \u001b[0;36mlinprog\u001b[1;34m(c, A_ub, b_ub, A_eq, b_eq, max_steps, t0, mu, eps, callback)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a solution to the given LP using the interior point method.\"\"\"\u001b[39;00m    \n\u001b[0;32m    249\u001b[0m x \u001b[38;5;241m=\u001b[39m linprog_feasible(A_ub, b_ub, A_eq, b_eq)  \u001b[38;5;66;03m# A_eq, b_eq deliberately omitted, since subsequent _linprog_ip is infeasible start Newton\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43m_linprog_ip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_ub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_ub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_eq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_eq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# Sanity check that equality constraints are satisfied.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# If the system is properly infeasible, it should have been caught by linprog_feasible raising an exception, so this is an internal check.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A_eq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\jairoescanez\\onedrive - universidad de sevilla\\uni\\5º\\tfg\\deep_inv_opt\\deep_inv_opt\\linprog.py:155\u001b[0m, in \u001b[0;36m_linprog_ip\u001b[1;34m(x, c, A_ub, b_ub, A_eq, b_eq, max_steps, t0, mu, eps, callback)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Checking stopping criterion of newton's method\u001b[39;00m\n\u001b[0;32m    154\u001b[0m lamb \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mt() \u001b[38;5;241m@\u001b[39m dxw   \n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lamb\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m eps:  \u001b[38;5;66;03m# Stopping criterion of newton's method\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     t \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m mu     \u001b[38;5;66;03m# Update t\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# Report current state. If callback returns True, terminate\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "deep_inv_opt.linprog(c, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_lp(c, A, b):\n",
    "    # x inicial aleatorio\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "\n",
    "    # Definir el optimizador\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "\n",
    "    # Optimizar para encontrar x que minimiza c^T x sujeto a Ax <= b\n",
    "    for _ in range(100):  # Pequeño número de pasos\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Penalizar violación de restricciones\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        \n",
    "        # Función objetivo relajada\n",
    "        objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "        objective.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return x.detach()\n",
    "\n",
    "def my_loss(output, target):\n",
    "    c = output[0:2]\n",
    "    A = output[2:6].reshape(2, 2)\n",
    "    b = output[6:8]\n",
    "\n",
    "    # Resolver el LP suavizado\n",
    "    rs = smooth_lp(c, A, b)\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000,  1.0000]), tensor([[1., 1.]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_lp(c, A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0000e+00, -6.5565e-07]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ smooth_lp(c, A, b) - b.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voy a probar a ver si funciona con la funcion de perdida de la competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Crear un dataset simple de vectores u\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Dataset de entrada (vectores u)\n",
    "u_data = [[1.0], [2.0], [3.0], [4.0]]\n",
    "dataset = UDataset(u_data)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return c, A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_lp(c, A, b):\n",
    "    # Inicializar x con gradientes habilitados\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "        objective.backward(retain_graph=True)  # Mantén el grafo activo\n",
    "        optimizer.step()\n",
    "    return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_loss(c, A, b, target):\n",
    "    rs = smooth_lp(c, A, b)\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.8327734470367432\n",
      "Epoch 10, Loss: 0.8327734470367432\n",
      "Epoch 20, Loss: 0.2942815124988556\n",
      "Epoch 30, Loss: 2.3538384437561035\n",
      "Epoch 40, Loss: 0.2942815124988556\n",
      "Epoch 50, Loss: 2.7194652557373047\n",
      "Epoch 60, Loss: 0.8327734470367432\n",
      "Epoch 70, Loss: 2.7194652557373047\n",
      "Epoch 80, Loss: 0.2942815124988556\n",
      "Epoch 90, Loss: 0.8327734470367432\n"
     ]
    }
   ],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Valores objetivo de ejemplo (target), en mi caso son los valores de x\n",
    "target = torch.tensor([1.0, 1.5])\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(100):\n",
    "    for u_batch in dataloader:\n",
    "        c, A, b = model(u_batch)\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = my_loss(c[0], A[0], b[0], target)\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo junto y con una buena estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Loss: 95.222305  [    0/   12]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Loss: 12.245973  [    0/   12]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Loss: 8.683558  [    0/   12]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Loss: 12.245973  [    0/   12]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Loss: 15.968550  [    0/   12]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Loss: 49.857544  [    0/   12]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Loss: 12.245973  [    0/   12]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Loss: 10.028285  [    0/   12]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Loss: 95.222305  [    0/   12]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Loss: 8.683558  [    0/   12]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Dataset simple de vectores u\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], torch.tensor([1.0, 1.5])  # El target es fijo en tu caso\n",
    "\n",
    "# Dataset y DataLoader\n",
    "u_data = [[1.0], [2.0], [3.0], [4.0], [5.0], [6.0], [7.0], [8.0], [9.0], [10.0], [11.0], [12.0]]\n",
    "# genero un dataset u aleatorio y grande en el que cada elemento es un tensor de 1x1\n",
    "# u_data = np.random.rand(1000, 1)\n",
    "# u_data = [[u] for u in u_data]\n",
    "\n",
    "train_dataloader = DataLoader(UDataset(u_data), batch_size=2, shuffle=True)\n",
    "\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)\n",
    "        b = output[:, 6:8]\n",
    "        return c, A, b\n",
    "\n",
    "def smooth_lp(c, A, b):\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "        objective.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "    return x\n",
    "\n",
    "def my_loss(c, A, b, target):\n",
    "    rs = smooth_lp(c, A, b)\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Entrenamiento\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (u_batch, target) in enumerate(dataloader):\n",
    "        c, A, b = model(u_batch)\n",
    "\n",
    "        # Calcular la pérdida\n",
    "        loss = loss_fn(c[0], A[0], b[0], target)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f\"Loss: {loss.item():>7f}  [{batch * len(u_batch):>5d}/{len(dataloader.dataset):>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u_batch, target in dataloader:\n",
    "            c, A, b = model(u_batch)\n",
    "            test_loss += loss_fn(c[0], A[0], b[0], target).item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    print(f\"Test Avg loss: {test_loss:>8f}\\n\")\n",
    "\n",
    "# Configuración\n",
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenamiento y evaluación\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, my_loss, optimizer)\n",
    "    # test_loop(train_dataloader, model, my_loss)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.],\n",
      "        [1.]])\n",
      "tensor([[2.],\n",
      "        [4.]])\n"
     ]
    }
   ],
   "source": [
    "for u in dataloader:\n",
    "    print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
