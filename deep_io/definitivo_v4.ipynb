{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5000],\n",
       "        [-1.4032],\n",
       "        [-1.3065],\n",
       "        [-1.2097],\n",
       "        [-1.1129],\n",
       "        [-1.0161],\n",
       "        [-0.9194],\n",
       "        [-0.8226],\n",
       "        [-0.7258],\n",
       "        [-0.6290],\n",
       "        [-0.5323],\n",
       "        [-0.4355],\n",
       "        [-0.3387],\n",
       "        [-0.2419],\n",
       "        [-0.1452],\n",
       "        [-0.0484],\n",
       "        [ 0.0484],\n",
       "        [ 0.1452],\n",
       "        [ 0.2419],\n",
       "        [ 0.3387],\n",
       "        [ 0.4355],\n",
       "        [ 0.5323],\n",
       "        [ 0.6290],\n",
       "        [ 0.7258],\n",
       "        [ 0.8226],\n",
       "        [ 0.9194],\n",
       "        [ 1.0161],\n",
       "        [ 1.1129],\n",
       "        [ 1.2097],\n",
       "        [ 1.3065],\n",
       "        [ 1.4032],\n",
       "        [ 1.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(-1.5, 1.5, 32).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5000],\n",
       "        [-1.3000],\n",
       "        [-1.1000],\n",
       "        [-0.9000],\n",
       "        [-0.7000],\n",
       "        [-0.5000],\n",
       "        [-0.3000],\n",
       "        [-0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.3000],\n",
       "        [ 0.5000],\n",
       "        [ 0.7000],\n",
       "        [ 0.9000],\n",
       "        [ 1.1000],\n",
       "        [ 1.3000],\n",
       "        [ 1.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_val = io.tensor(np.linspace(-1.5, 1.5, 16).reshape((-1, 1)))\n",
    "u_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w[0] + w[1]*u)],\n",
    "             [torch.sin(w[0] + w[1]*u)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],\n",
    "                [ 0.0, -1.0],\n",
    "                [ w[0], 1 + w[1]*u/3]]\n",
    "\n",
    "        b_ub = [[ 0.2*w[0]*u],\n",
    "                [-0.2*w[1]*u],\n",
    "                [ w[0] + 0.1*u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[1.0, 1.0])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "torch.save(x_train, \"x_train.pt\")\n",
    "\n",
    "x_val = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_val])\n",
    "torch.save(x_val, \"x_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000,  1.1000],\n",
       "        [ 0.2600,  1.0764],\n",
       "        [ 0.2200,  1.0579],\n",
       "        [ 0.1800, -0.1800],\n",
       "        [ 0.1400, -0.1400],\n",
       "        [ 0.1000, -0.1000],\n",
       "        [ 0.0600, -0.0600],\n",
       "        [ 0.0200, -0.0200],\n",
       "        [-0.0200,  0.0200],\n",
       "        [-0.0600,  0.0600],\n",
       "        [-0.1000,  0.1000],\n",
       "        [ 0.8973,  0.1400],\n",
       "        [ 0.8560,  0.1800],\n",
       "        [ 0.8093,  0.2200],\n",
       "        [ 0.7573,  0.2600],\n",
       "        [ 0.7000,  0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = torch.load(\"x_val.pt\")\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000,  1.1000],\n",
       "        [ 0.2806,  1.0879],\n",
       "        [ 0.2613,  1.0771],\n",
       "        [ 0.2419,  1.0675],\n",
       "        [ 0.2226,  1.0589],\n",
       "        [ 0.2032,  1.0509],\n",
       "        [ 0.1839, -0.1838],\n",
       "        [ 0.1645, -0.1645],\n",
       "        [ 0.1452, -0.1451],\n",
       "        [ 0.1258, -0.1258],\n",
       "        [ 0.1065, -0.1064],\n",
       "        [ 0.0871, -0.0871],\n",
       "        [ 0.0677, -0.0677],\n",
       "        [ 0.0484, -0.0484],\n",
       "        [ 0.0290, -0.0290],\n",
       "        [ 0.0097, -0.0097],\n",
       "        [-0.0097,  0.0097],\n",
       "        [-0.0290,  0.0290],\n",
       "        [-0.0484,  0.0484],\n",
       "        [-0.0677,  0.0677],\n",
       "        [-0.0871,  0.0871],\n",
       "        [-0.1063,  0.1065],\n",
       "        [ 0.9106,  0.1258],\n",
       "        [ 0.8923,  0.1452],\n",
       "        [ 0.8726,  0.1645],\n",
       "        [ 0.8517,  0.1839],\n",
       "        [ 0.8295,  0.2032],\n",
       "        [ 0.8061,  0.2226],\n",
       "        [ 0.7815,  0.2419],\n",
       "        [ 0.7556,  0.2613],\n",
       "        [ 0.7284,  0.2806],\n",
       "        [ 0.7000,  0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# ver si estoy usando GPU\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = UDataset(u_val, x_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos, pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe realizarse con operaciones de pytorch para\n",
    "# preservar el grafo de computo para poder hacer backpropagation\n",
    "# def smooth_lp(c, A, b):\n",
    "#     # Inicializar x con gradientes habilitados\n",
    "#     x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "#     optimizer = torch.optim.SGD([x], lr=1e-3)\n",
    "\n",
    "#     for _ in range(1000):\n",
    "#         optimizer.zero_grad()\n",
    "#         constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "#         objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "#         objective.backward(retain_graph=True)  # Mantén el grafo activo\n",
    "#         optimizer.step()\n",
    "#     return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy descartado porque no funciona\n",
    "# voy a probar con cvxpy\n",
    "from diffcp import SolverError\n",
    "import torch\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "def solver(c_, A_, b_):\n",
    "    m,n = A_.shape\n",
    "\n",
    "    A = cp.Parameter((m, n))\n",
    "    b = cp.Parameter(m)\n",
    "    c = cp.Parameter(n)\n",
    "    x = cp.Variable(n)\n",
    "\n",
    "    obj = cp.Minimize(c.T @ x)\n",
    "    cons = [ A @ x <= b ]\n",
    "    prob = cp.Problem(obj, cons)\n",
    "\n",
    "    layer = CvxpyLayer(prob, parameters=[c,A,b], variables=[x])\n",
    "\n",
    "    # solution, = layer(c_, A_, b_)\n",
    "    # return solution\n",
    "    try:\n",
    "        solution, = layer(c_, A_, b_)\n",
    "        return solution\n",
    "    except Exception as e:\n",
    "        # print(\"El problema no tiene solución o es numéricamente inestable.\")\n",
    "        return torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([1.0, -1.0])\n",
    "\n",
    "A = torch.tensor([[1.0, 0.0],\n",
    "                  [0.0, 1.0]])\n",
    "b = torch.tensor([0.0, 0.0])\n",
    "\n",
    "print(solver(c, A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta version es la red neuronal en la que la resolucion del problema de optimizacion esta dentro de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qué)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return c, A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(cs, As, bs, X_true):\n",
    "    '''\n",
    "    Función de pérdida para la red neuronal.\n",
    "\n",
    "    Args:\n",
    "        cs: Tensor de costes.\n",
    "        As: Tensor de matrices A.\n",
    "        bs: Tensor de vectores b.\n",
    "        X_true: Tensor de valores verdaderos.\n",
    "    '''\n",
    "    loss = 0.0\n",
    "    for c, A, b, x in zip(cs, As, bs, X_true):\n",
    "        x_pred = solver(c, A, b)\n",
    "        loss += torch.mean((x_pred - x)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # elegir una de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver el modelo\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamod los pesos de la red \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu') # inicializamos los pesos de la red con Kaiming\n",
    "        nn.init.zeros_(m.bias) # inicializamos los bias a 0\n",
    "\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near(rs, x_batch, tol=1e-3):\n",
    "    '''\n",
    "    rs: tensor formado por las soluciones de los LPs\n",
    "    x_batch: tensor formado por las soluciones deseadas de los LPs\n",
    "    tol: tolerancia para considerar que dos vectores son iguales\n",
    "\n",
    "    Esta función calcula el número de soluciones de los LPs que están \n",
    "    a una distancia menor que tol de las soluciones deseadas.\n",
    "    '''\n",
    "    matriz_diferencias = rs - x_batch # matriz de diferencias entre las soluciones de los LPs y las soluciones deseadas\n",
    "    matriz_al_cuadrado = matriz_diferencias ** 2 # elevo cada elemento al cuadrado\n",
    "    distancias = torch.sum(matriz_al_cuadrado, dim=1) # sumo los elementos de cada fila\n",
    "    return torch.sum(distancias < tol).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss_media = 0.0\n",
    "    accuracy = 0.0\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "\n",
    "        rs = model(u_batch)\n",
    "        loss = loss_fn(*rs, x_batch) # Calcular la pérdida\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        loss.backward() # Calcular los gradientes\n",
    "        optimizer.step() # Actualizar los pesos del modelo\n",
    "        optimizer.zero_grad() # Reiniciar los gradientes\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss = loss.item()\n",
    "            current = batch * batch_size + batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        loss_media += loss\n",
    "        # accuracy += near(rs, x_batch, tol)\n",
    "\n",
    "    return loss_media / num_batches, accuracy / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        rs = model(u_batch)\n",
    "        test_loss += loss_fn(rs, x_batch).item()\n",
    "        # correct += near(rs, x_batch, tol) # consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "loss: 0.027155  [    2/   32]\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "loss: 1.014696  [   18/   32]\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "\n",
      "Training Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.460763 \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loss_fn() missing 2 required positional arguments: 'bs' and 'X_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m train_loop(dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Error: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccuracy)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>0.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ahora lo que esta pasando es que el modelo devuelve unos parametros del problema que no tienen sentido, por lo que la solucion no es correcta\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[95], line 10\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u_batch, x_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m     rs \u001b[38;5;241m=\u001b[39m model(u_batch)\n\u001b[0;32m---> 10\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# correct += near(rs, x_batch, tol) # consideramos correcto si se acerca a la solucion en la distancia euclidea \u001b[39;00m\n\u001b[1;32m     12\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_fn() missing 2 required positional arguments: 'bs' and 'X_true'"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 8\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss, accuracy = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"\\nTraining Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    \n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "# ahora lo que esta pasando es que el modelo devuelve unos parametros del problema que no tienen sentido, por lo que la solucion no es correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0200,  0.0200],\n",
       "        [-0.0720,  0.0720],\n",
       "        [ 0.9123,  0.1240],\n",
       "        [ 0.8604,  0.1760],\n",
       "        [ 0.7993,  0.2280],\n",
       "        [ 0.7293,  0.2800],\n",
       "        [ 0.6503,  0.3320],\n",
       "        [ 0.5622,  0.3840],\n",
       "        [ 0.4652,  0.4360],\n",
       "        [ 0.3591,  0.4880],\n",
       "        [ 0.2440,  0.5400],\n",
       "        [ 0.1199,  0.5920],\n",
       "        [-0.0133,  0.6440],\n",
       "        [-0.6960,  0.9463],\n",
       "        [-0.7480,  0.9445],\n",
       "        [-0.8000,  0.9429]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[-0.0500,  0.0500]], dtype=torch.float64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = model(torch.tensor([[0.25]]))\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0500,  0.0500]], dtype=torch.float64),\n",
       " tensor([[0., 0.]], grad_fn=<StackBackward0>),\n",
       " tensor(0.0707, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar:\n",
    "- Función de pérdida ✔️\n",
    "- Función smooth_lp (a ver si puedo sustituir la funcion smooth_lp por la funcion de linprog de deep_inv_opt)\n",
    "- Organizar y explicar el código\n",
    "- Estudiar por qué no disminuye el loss cuando entreno el modelo: es porque está mostrando el loss en el conjunto de test y no en el de entrenamiento ✔️\n",
    "- Entrenar el modelo con más épocas y datos\n",
    "- Pensar qué optimizador es mejor ✔️\n",
    "- Pensar qué estructura de la red es mejor ✔️\n",
    "- Inicialización de los pesos de la red ✔️\n",
    "- Arreglar el porcentaje de aciertos ✔️\n",
    "\n",
    "\n",
    "\n",
    "Bitácora: ahora lo que pasa es que da error porque el problema de optimización no está acotado, lo que tengo que hacer es revisar en el paper el intervalo donde se mueve la u porque creo que lo he puesto mal. Cuando arregle esto, tengo que volver a entrenar el modelo y ver si se está entrenando bien. También tengo que probar resolviendo el problema de optimización con la función de linprog de deep_inv_opt y con la de cvxpy a ver si alguna de las dos funciona mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
