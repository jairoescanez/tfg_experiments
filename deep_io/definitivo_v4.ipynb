{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5000],\n",
       "        [-1.4032],\n",
       "        [-1.3065],\n",
       "        [-1.2097],\n",
       "        [-1.1129],\n",
       "        [-1.0161],\n",
       "        [-0.9194],\n",
       "        [-0.8226],\n",
       "        [-0.7258],\n",
       "        [-0.6290],\n",
       "        [-0.5323],\n",
       "        [-0.4355],\n",
       "        [-0.3387],\n",
       "        [-0.2419],\n",
       "        [-0.1452],\n",
       "        [-0.0484],\n",
       "        [ 0.0484],\n",
       "        [ 0.1452],\n",
       "        [ 0.2419],\n",
       "        [ 0.3387],\n",
       "        [ 0.4355],\n",
       "        [ 0.5323],\n",
       "        [ 0.6290],\n",
       "        [ 0.7258],\n",
       "        [ 0.8226],\n",
       "        [ 0.9194],\n",
       "        [ 1.0161],\n",
       "        [ 1.1129],\n",
       "        [ 1.2097],\n",
       "        [ 1.3065],\n",
       "        [ 1.4032],\n",
       "        [ 1.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(-1.5, 1.5, 32).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5000],\n",
       "        [-1.3000],\n",
       "        [-1.1000],\n",
       "        [-0.9000],\n",
       "        [-0.7000],\n",
       "        [-0.5000],\n",
       "        [-0.3000],\n",
       "        [-0.1000],\n",
       "        [ 0.1000],\n",
       "        [ 0.3000],\n",
       "        [ 0.5000],\n",
       "        [ 0.7000],\n",
       "        [ 0.9000],\n",
       "        [ 1.1000],\n",
       "        [ 1.3000],\n",
       "        [ 1.5000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_val = io.tensor(np.linspace(-1.5, 1.5, 16).reshape((-1, 1)))\n",
    "u_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w[0] + w[1]*u)],\n",
    "             [torch.sin(w[0] + w[1]*u)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],\n",
    "                [ 0.0, -1.0],\n",
    "                [ w[0], 1 + w[1]*u/3]]\n",
    "\n",
    "        b_ub = [[ 0.2*w[0]*u],\n",
    "                [-0.2*w[1]*u],\n",
    "                [ w[0] + 0.1*u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[1.0, 1.0])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "torch.save(x_train, \"x_train.pt\")\n",
    "\n",
    "x_val = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_val])\n",
    "torch.save(x_val, \"x_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000,  1.1000],\n",
       "        [ 0.2600,  1.0764],\n",
       "        [ 0.2200,  1.0579],\n",
       "        [ 0.1800, -0.1800],\n",
       "        [ 0.1400, -0.1400],\n",
       "        [ 0.1000, -0.1000],\n",
       "        [ 0.0600, -0.0600],\n",
       "        [ 0.0200, -0.0200],\n",
       "        [-0.0200,  0.0200],\n",
       "        [-0.0600,  0.0600],\n",
       "        [-0.1000,  0.1000],\n",
       "        [ 0.8973,  0.1400],\n",
       "        [ 0.8560,  0.1800],\n",
       "        [ 0.8093,  0.2200],\n",
       "        [ 0.7573,  0.2600],\n",
       "        [ 0.7000,  0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = torch.load(\"x_val.pt\")\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3000,  1.1000],\n",
       "        [ 0.2806,  1.0879],\n",
       "        [ 0.2613,  1.0771],\n",
       "        [ 0.2419,  1.0675],\n",
       "        [ 0.2226,  1.0589],\n",
       "        [ 0.2032,  1.0509],\n",
       "        [ 0.1839, -0.1838],\n",
       "        [ 0.1645, -0.1645],\n",
       "        [ 0.1452, -0.1451],\n",
       "        [ 0.1258, -0.1258],\n",
       "        [ 0.1065, -0.1064],\n",
       "        [ 0.0871, -0.0871],\n",
       "        [ 0.0677, -0.0677],\n",
       "        [ 0.0484, -0.0484],\n",
       "        [ 0.0290, -0.0290],\n",
       "        [ 0.0097, -0.0097],\n",
       "        [-0.0097,  0.0097],\n",
       "        [-0.0290,  0.0290],\n",
       "        [-0.0484,  0.0484],\n",
       "        [-0.0677,  0.0677],\n",
       "        [-0.0871,  0.0871],\n",
       "        [-0.1063,  0.1065],\n",
       "        [ 0.9106,  0.1258],\n",
       "        [ 0.8923,  0.1452],\n",
       "        [ 0.8726,  0.1645],\n",
       "        [ 0.8517,  0.1839],\n",
       "        [ 0.8295,  0.2032],\n",
       "        [ 0.8061,  0.2226],\n",
       "        [ 0.7815,  0.2419],\n",
       "        [ 0.7556,  0.2613],\n",
       "        [ 0.7284,  0.2806],\n",
       "        [ 0.7000,  0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# ver si estoy usando GPU\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = UDataset(u_val, x_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos, pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe realizarse con operaciones de pytorch para\n",
    "# preservar el grafo de computo para poder hacer backpropagation\n",
    "# def smooth_lp(c, A, b):\n",
    "#     # Inicializar x con gradientes habilitados\n",
    "#     x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "#     optimizer = torch.optim.SGD([x], lr=1e-3)\n",
    "\n",
    "#     for _ in range(1000):\n",
    "#         optimizer.zero_grad()\n",
    "#         constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "#         objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "#         objective.backward(retain_graph=True)  # Mant√©n el grafo activo\n",
    "#         optimizer.step()\n",
    "#     return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy descartado porque no funciona\n",
    "# voy a probar con cvxpy\n",
    "from diffcp import SolverError\n",
    "import torch\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "\n",
    "def solver(c_, A_, b_):\n",
    "    m,n = A_.shape\n",
    "\n",
    "    A = cp.Parameter((m, n))\n",
    "    b = cp.Parameter(m)\n",
    "    c = cp.Parameter(n)\n",
    "    x = cp.Variable(n)\n",
    "\n",
    "    obj = cp.Minimize(c.T @ x)\n",
    "    cons = [ A @ x <= b ]\n",
    "    prob = cp.Problem(obj, cons)\n",
    "\n",
    "    layer = CvxpyLayer(prob, parameters=[c,A,b], variables=[x])\n",
    "\n",
    "    # solution, = layer(c_, A_, b_)\n",
    "    # return solution\n",
    "    try:\n",
    "        solution, = layer(c_, A_, b_)\n",
    "        return solution\n",
    "    except Exception as e:\n",
    "        # print(\"El problema no tiene soluci√≥n o es num√©ricamente inestable.\")\n",
    "        return torch.tensor([0.0, 0.0], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "tensor([0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([1.0, -1.0])\n",
    "\n",
    "A = torch.tensor([[1.0, 0.0],\n",
    "                  [0.0, 1.0]])\n",
    "b = torch.tensor([0.0, 0.0])\n",
    "\n",
    "print(solver(c, A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta version es la red neuronal en la que la resolucion del problema de optimizacion esta dentro de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qu√©)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensi√≥n 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return c, A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(cs, As, bs, X_true):\n",
    "    '''\n",
    "    Funci√≥n de p√©rdida para la red neuronal.\n",
    "\n",
    "    Args:\n",
    "        cs: Tensor de costes.\n",
    "        As: Tensor de matrices A.\n",
    "        bs: Tensor de vectores b.\n",
    "        X_true: Tensor de valores verdaderos.\n",
    "    '''\n",
    "    loss = 0.0\n",
    "    for c, A, b, x in zip(cs, As, bs, X_true):\n",
    "        x_pred = solver(c, A, b)\n",
    "        loss += torch.mean((x_pred - x)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # elegir una de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver el modelo\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamod los pesos de la red \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu') # inicializamos los pesos de la red con Kaiming\n",
    "        nn.init.zeros_(m.bias) # inicializamos los bias a 0\n",
    "\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near(rs, x_batch, tol=1e-3):\n",
    "    '''\n",
    "    rs: tensor formado por las soluciones de los LPs\n",
    "    x_batch: tensor formado por las soluciones deseadas de los LPs\n",
    "    tol: tolerancia para considerar que dos vectores son iguales\n",
    "\n",
    "    Esta funci√≥n calcula el n√∫mero de soluciones de los LPs que est√°n \n",
    "    a una distancia menor que tol de las soluciones deseadas.\n",
    "    '''\n",
    "    matriz_diferencias = rs - x_batch # matriz de diferencias entre las soluciones de los LPs y las soluciones deseadas\n",
    "    matriz_al_cuadrado = matriz_diferencias ** 2 # elevo cada elemento al cuadrado\n",
    "    distancias = torch.sum(matriz_al_cuadrado, dim=1) # sumo los elementos de cada fila\n",
    "    return torch.sum(distancias < tol).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 1\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss_media = 0.0\n",
    "    accuracy = 0.0\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "\n",
    "        rs = model(u_batch)\n",
    "        loss = loss_fn(*rs, x_batch) # Calcular la p√©rdida\n",
    "\n",
    "        # Backpropagation y optimizaci√≥n\n",
    "        loss.backward() # Calcular los gradientes\n",
    "        optimizer.step() # Actualizar los pesos del modelo\n",
    "        optimizer.zero_grad() # Reiniciar los gradientes\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss = loss.item()\n",
    "            current = batch * batch_size + batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        loss_media += loss\n",
    "        # accuracy += near(rs, x_batch, tol)\n",
    "\n",
    "    return loss_media / num_batches, accuracy / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        rs = model(u_batch)\n",
    "        test_loss += loss_fn(rs, x_batch).item()\n",
    "        # correct += near(rs, x_batch, tol) # consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "loss: 0.027155  [    2/   32]\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "loss: 1.014696  [   18/   32]\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n",
      "\n",
      "Training Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.460763 \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "loss_fn() missing 2 required positional arguments: 'bs' and 'X_true'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m train_loop(dataloader, model, loss_fn, optimizer)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Error: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccuracy)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>0.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtest_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ahora lo que esta pasando es que el modelo devuelve unos parametros del problema que no tienen sentido, por lo que la solucion no es correcta\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[95], line 10\u001b[0m, in \u001b[0;36mtest_loop\u001b[0;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u_batch, x_batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      9\u001b[0m     rs \u001b[38;5;241m=\u001b[39m model(u_batch)\n\u001b[0;32m---> 10\u001b[0m     test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# correct += near(rs, x_batch, tol) # consideramos correcto si se acerca a la solucion en la distancia euclidea \u001b[39;00m\n\u001b[1;32m     12\u001b[0m test_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_fn() missing 2 required positional arguments: 'bs' and 'X_true'"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 8\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss, accuracy = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"\\nTraining Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    \n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "# ahora lo que esta pasando es que el modelo devuelve unos parametros del problema que no tienen sentido, por lo que la solucion no es correcta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0200,  0.0200],\n",
       "        [-0.0720,  0.0720],\n",
       "        [ 0.9123,  0.1240],\n",
       "        [ 0.8604,  0.1760],\n",
       "        [ 0.7993,  0.2280],\n",
       "        [ 0.7293,  0.2800],\n",
       "        [ 0.6503,  0.3320],\n",
       "        [ 0.5622,  0.3840],\n",
       "        [ 0.4652,  0.4360],\n",
       "        [ 0.3591,  0.4880],\n",
       "        [ 0.2440,  0.5400],\n",
       "        [ 0.1199,  0.5920],\n",
       "        [-0.0133,  0.6440],\n",
       "        [-0.6960,  0.9463],\n",
       "        [-0.7480,  0.9445],\n",
       "        [-0.8000,  0.9429]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[-0.0500,  0.0500]], dtype=torch.float64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please consider re-formulating your problem so that it is always solvable or increasing the number of solver iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = model(torch.tensor([[0.25]]))\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0500,  0.0500]], dtype=torch.float64),\n",
       " tensor([[0., 0.]], grad_fn=<StackBackward0>),\n",
       " tensor(0.0707, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar:\n",
    "- Funci√≥n de p√©rdida ‚úîÔ∏è\n",
    "- Funci√≥n smooth_lp (a ver si puedo sustituir la funcion smooth_lp por la funcion de linprog de deep_inv_opt)\n",
    "- Organizar y explicar el c√≥digo\n",
    "- Estudiar por qu√© no disminuye el loss cuando entreno el modelo: es porque est√° mostrando el loss en el conjunto de test y no en el de entrenamiento ‚úîÔ∏è\n",
    "- Entrenar el modelo con m√°s √©pocas y datos\n",
    "- Pensar qu√© optimizador es mejor ‚úîÔ∏è\n",
    "- Pensar qu√© estructura de la red es mejor ‚úîÔ∏è\n",
    "- Inicializaci√≥n de los pesos de la red ‚úîÔ∏è\n",
    "- Arreglar el porcentaje de aciertos ‚úîÔ∏è\n",
    "\n",
    "\n",
    "\n",
    "Bit√°cora: ahora lo que pasa es que da error porque el problema de optimizaci√≥n no est√° acotado, lo que tengo que hacer es revisar en el paper el intervalo donde se mueve la u porque creo que lo he puesto mal. Cuando arregle esto, tengo que volver a entrenar el modelo y ver si se est√° entrenando bien. Tambi√©n tengo que probar resolviendo el problema de optimizaci√≥n con la funci√≥n de linprog de deep_inv_opt y con la de cvxpy a ver si alguna de las dos funciona mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
