{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.5484],\n",
       "        [ 0.5968],\n",
       "        ...,\n",
       "        [49.9032],\n",
       "        [49.9516],\n",
       "        [50.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(0.5, 50, 1024).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w + u**2 / 2)],\n",
    "             [torch.sin(w + u**2 / 2)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],      # x1 >= 0\n",
    "                [ 0.0, -1.0],      # x2 >= 0\n",
    "                [ 1.0,  0.0],      # x1 <= 2*w\n",
    "                [ .5*w, w]]  # (1+w)*x1 + 2*(1+w)*x2 <= u\n",
    "\n",
    "        b_ub = [[ 0.0],\n",
    "                [ 0.0],\n",
    "                [ 4/u],\n",
    "                [   u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[0.8])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "# x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "# torch.save(x_train, \"x_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3378e-06, 4.7761e-06],\n",
       "        [6.4988e-06, 4.6438e-06],\n",
       "        [6.1619e-06, 4.1501e-06],\n",
       "        ...,\n",
       "        [8.0112e-02, 1.4527e-05],\n",
       "        [1.9709e-04, 6.2439e+01],\n",
       "        [2.5363e-05, 5.3045e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "# u_data = [[1.0], [2.0], [3.0], [4.0]]\n",
    "# x_targets = [[1.0, 1.5], [2.0, 2.5], [3.0, 3.5], [4.0, 4.5]]\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qué)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return c, A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe preservar el grafo de computo para poder hacer backpropagation\n",
    "def smooth_lp(c, A, b):\n",
    "    # Inicializar x con gradientes habilitados\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD([x], lr=0.01)\n",
    "\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "        objective.backward(retain_graph=True)  # Mantén el grafo activo\n",
    "        optimizer.step()\n",
    "    return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de perdida\n",
    "def my_loss(c, A, b, target):\n",
    "    rs = smooth_lp(c, A, b)\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn(c, A, b, target):\n",
    "    return torch.mean(torch.stack([my_loss(c[i], A[i], b[i], target[i]) for i in range(len(c))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "\n",
    "        c, A, b = model(u_batch)\n",
    "        # Calcular la pérdida\n",
    "        #loss = my_loss(c[0], A[0], b[0], x_batch[0])  # Usar el target correspondiente\n",
    "        loss = loss_fn(c, A, b, x_batch)\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss, current = loss.item(), batch * batch_size + len(u_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        c, A, b = model(u_batch)\n",
    "        test_loss += loss_fn(c, A, b, x_batch).item()\n",
    "        x_pred = torch.stack([smooth_lp(c[i], A[i], b[i]) for i in range(len(c))])\n",
    "        correct += torch.sum(x_pred == x_batch).item() # cambiar esto, aqui poner la solucion del problema\n",
    "        # y consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 308.633636  [    8/ 1024]\n",
      "loss: 44.316528  [   72/ 1024]\n",
      "loss: 645.018982  [  136/ 1024]\n",
      "loss: 702.106567  [  200/ 1024]\n",
      "loss: 600.218506  [  264/ 1024]\n",
      "loss: 189.065430  [  328/ 1024]\n",
      "loss: 293.191162  [  392/ 1024]\n",
      "loss: 176.093033  [  456/ 1024]\n",
      "loss: 97.444130  [  520/ 1024]\n",
      "loss: 387.624969  [  584/ 1024]\n",
      "loss: 312.448669  [  648/ 1024]\n",
      "loss: 467.504272  [  712/ 1024]\n",
      "loss: 457.937561  [  776/ 1024]\n",
      "loss: 137.024902  [  840/ 1024]\n",
      "loss: 414.401276  [  904/ 1024]\n",
      "loss: 177.858109  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 426.603010 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 213.265717  [    8/ 1024]\n",
      "loss: 153.985718  [   72/ 1024]\n",
      "loss: 170.831665  [  136/ 1024]\n",
      "loss: 406.683990  [  200/ 1024]\n",
      "loss: 560.645142  [  264/ 1024]\n",
      "loss: 369.410431  [  328/ 1024]\n",
      "loss: 446.683563  [  392/ 1024]\n",
      "loss: 568.539795  [  456/ 1024]\n",
      "loss: 141.295364  [  520/ 1024]\n",
      "loss: 500.543060  [  584/ 1024]\n",
      "loss: 683.219604  [  648/ 1024]\n",
      "loss: 740.996338  [  712/ 1024]\n",
      "loss: 536.030212  [  776/ 1024]\n",
      "loss: 711.007080  [  840/ 1024]\n",
      "loss: 373.403168  [  904/ 1024]\n",
      "loss: 526.780457  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 426.603009 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 709.766235  [    8/ 1024]\n",
      "loss: 382.449341  [   72/ 1024]\n",
      "loss: 323.314240  [  136/ 1024]\n",
      "loss: 390.476013  [  200/ 1024]\n",
      "loss: 645.060974  [  264/ 1024]\n",
      "loss: 436.589111  [  328/ 1024]\n",
      "loss: 376.938507  [  392/ 1024]\n",
      "loss: 438.458221  [  456/ 1024]\n",
      "loss: 500.322693  [  520/ 1024]\n",
      "loss: 618.973755  [  584/ 1024]\n",
      "loss: 349.332916  [  648/ 1024]\n",
      "loss: 693.901367  [  712/ 1024]\n",
      "loss: 254.097900  [  776/ 1024]\n",
      "loss: 524.486755  [  840/ 1024]\n",
      "loss: 375.093689  [  904/ 1024]\n",
      "loss: 651.550171  [  968/ 1024]\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 4 # poner mas\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0239, -0.0908], requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c,A,b=model(torch.tensor([[1.0]]))\n",
    "smooth_lp(c[0], A[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `validacion` not found.\n"
     ]
    }
   ],
   "source": [
    "en el dataloader hay alguna manera de añadirle un conjunto de validacion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5031e-06, 5.2914e-06],\n",
       "        [5.8791e-06, 5.0125e-06],\n",
       "        [6.2946e-06, 4.1114e-06],\n",
       "        [1.5288e-05, 6.1703e-06],\n",
       "        [6.3201e-05, 7.6848e-06],\n",
       "        [2.8571e+00, 7.7249e-06],\n",
       "        [2.4096e+00, 8.3817e-06],\n",
       "        [2.0833e+00, 1.4402e-05],\n",
       "        [1.8349e+00, 1.8074e+00],\n",
       "        [1.6393e+00, 2.2303e+00],\n",
       "        [6.9978e-05, 3.3749e+00],\n",
       "        [1.2749e-05, 3.7000e+00],\n",
       "        [6.8515e-06, 4.0250e+00],\n",
       "        [1.1909e-05, 1.8498e-05],\n",
       "        [2.2888e-04, 1.3794e-05],\n",
       "        [9.9999e-01, 1.9565e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0170,  0.1616]], grad_fn=<SliceBackward0>),\n",
       " tensor([[[ 0.1045,  0.2215],\n",
       "          [ 0.1905, -0.0086]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[0.4271, 0.0514]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "c, A, b = model(torch.tensor([[0.25]]))\n",
    "x_pred = smooth_lp(c[0], A[0], b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64),\n",
       " tensor([ 0.0170, -0.1616], requires_grad=True),\n",
       " tensor(0.1625, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se parecen bastante, ahora el siguiente paso es hacer la validacion, ya lo dejo para mañana"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
