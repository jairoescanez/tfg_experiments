{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.5484],\n",
       "        [ 0.5968],\n",
       "        ...,\n",
       "        [49.9032],\n",
       "        [49.9516],\n",
       "        [50.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(0.5, 50, 1024).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w + u**2 / 2)],\n",
    "             [torch.sin(w + u**2 / 2)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],      # x1 >= 0\n",
    "                [ 0.0, -1.0],      # x2 >= 0\n",
    "                [ 1.0,  0.0],      # x1 <= 2*w\n",
    "                [ .5*w, w]]  # (1+w)*x1 + 2*(1+w)*x2 <= u\n",
    "\n",
    "        b_ub = [[ 0.0],\n",
    "                [ 0.0],\n",
    "                [ 4/u],\n",
    "                [   u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[0.8])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "# x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "# torch.save(x_train, \"x_train.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3378e-06, 4.7761e-06],\n",
       "        [6.4988e-06, 4.6438e-06],\n",
       "        [6.1619e-06, 4.1501e-06],\n",
       "        ...,\n",
       "        [8.0112e-02, 1.4527e-05],\n",
       "        [1.9709e-04, 6.2439e+01],\n",
       "        [2.5363e-05, 5.3045e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta version es la red neuronal en la que la resolucion del problema de optimizacion esta dentro de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "# u_data = [[1.0], [2.0], [3.0], [4.0]]\n",
    "# x_targets = [[1.0, 1.5], [2.0, 2.5], [3.0, 3.5], [4.0, 4.5]]\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe preservar el grafo de computo para poder hacer backpropagation\n",
    "def smooth_lp(c, A, b):\n",
    "    # Inicializar x con gradientes habilitados\n",
    "    x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "\n",
    "    optimizer = torch.optim.SGD([x], lr=learning_rate)\n",
    "\n",
    "    for _ in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "        objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "        objective.backward(retain_graph=True)  # Mantén el grafo activo\n",
    "        optimizer.step()\n",
    "    return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qué)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return torch.stack([smooth_lp(c[i], A[i], b[i]) for i in range(u.shape[0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de perdida\n",
    "def my_loss(rs, target):\n",
    "    loss = torch.sum((rs - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "def loss_fn(rs, target):\n",
    "    return torch.mean(torch.stack([my_loss(rs[i], target[i]) for i in range(len(rs))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # elegir una de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "        size = len(dataloader.dataset)\n",
    "\n",
    "        rs = model(u_batch)\n",
    "        # Calcular la pérdida\n",
    "        #loss = my_loss(c[0], A[0], b[0], x_batch[0])  # Usar el target correspondiente\n",
    "        loss = loss_fn(rs, x_batch)\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss, current = loss.item(), batch * batch_size + len(u_batch)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        rs = model(u_batch)\n",
    "        test_loss += loss_fn(rs, x_batch).item()\n",
    "        correct += torch.sum(rs == x_batch).item() # cambiar esto, aqui poner la solucion del problema\n",
    "        # y consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 485.205322  [    8/ 1024]\n",
      "loss: 244.984070  [   72/ 1024]\n",
      "loss: 804.971008  [  136/ 1024]\n",
      "loss: 269.654266  [  200/ 1024]\n",
      "loss: 693.579712  [  264/ 1024]\n",
      "loss: 118.670303  [  328/ 1024]\n",
      "loss: 679.134521  [  392/ 1024]\n",
      "loss: 1408.530518  [  456/ 1024]\n",
      "loss: 385.504852  [  520/ 1024]\n",
      "loss: 828.076111  [  584/ 1024]\n",
      "loss: 510.319061  [  648/ 1024]\n",
      "loss: 461.941101  [  712/ 1024]\n",
      "loss: 484.007141  [  776/ 1024]\n",
      "loss: 510.091003  [  840/ 1024]\n",
      "loss: 638.853699  [  904/ 1024]\n",
      "loss: 523.650391  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 651.517210 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 714.963745  [    8/ 1024]\n",
      "loss: 903.632446  [   72/ 1024]\n",
      "loss: 1251.721313  [  136/ 1024]\n",
      "loss: 1229.534790  [  200/ 1024]\n",
      "loss: 1098.389160  [  264/ 1024]\n",
      "loss: 826.662537  [  328/ 1024]\n",
      "loss: 405.439301  [  392/ 1024]\n",
      "loss: 1071.989258  [  456/ 1024]\n",
      "loss: 376.599243  [  520/ 1024]\n",
      "loss: 657.246155  [  584/ 1024]\n",
      "loss: 549.175964  [  648/ 1024]\n",
      "loss: 1256.094482  [  712/ 1024]\n",
      "loss: 82.234024  [  776/ 1024]\n",
      "loss: 971.712280  [  840/ 1024]\n",
      "loss: 816.485474  [  904/ 1024]\n",
      "loss: 961.377319  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 651.517206 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 304.617188  [    8/ 1024]\n",
      "loss: 1531.706787  [   72/ 1024]\n",
      "loss: 657.242493  [  136/ 1024]\n",
      "loss: 665.507690  [  200/ 1024]\n",
      "loss: 280.219757  [  264/ 1024]\n",
      "loss: 572.227234  [  328/ 1024]\n",
      "loss: 438.514343  [  392/ 1024]\n",
      "loss: 1109.142090  [  456/ 1024]\n",
      "loss: 408.857269  [  520/ 1024]\n",
      "loss: 101.497643  [  584/ 1024]\n",
      "loss: 177.196045  [  648/ 1024]\n",
      "loss: 1068.771851  [  712/ 1024]\n",
      "loss: 503.106232  [  776/ 1024]\n",
      "loss: 1012.801758  [  840/ 1024]\n",
      "loss: 146.501114  [  904/ 1024]\n",
      "loss: 1377.078125  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 651.517207 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 919.941895  [    8/ 1024]\n",
      "loss: 571.950867  [   72/ 1024]\n",
      "loss: 40.923874  [  136/ 1024]\n",
      "loss: 907.074158  [  200/ 1024]\n",
      "loss: 884.398438  [  264/ 1024]\n",
      "loss: 376.128418  [  328/ 1024]\n",
      "loss: 846.528564  [  392/ 1024]\n",
      "loss: 541.538574  [  456/ 1024]\n",
      "loss: 642.417175  [  520/ 1024]\n",
      "loss: 607.079712  [  584/ 1024]\n",
      "loss: 277.416473  [  648/ 1024]\n",
      "loss: 619.769897  [  712/ 1024]\n",
      "loss: 227.210358  [  776/ 1024]\n",
      "loss: 527.240723  [  840/ 1024]\n",
      "loss: 187.710022  [  904/ 1024]\n",
      "loss: 611.597717  [  968/ 1024]\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 651.517207 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 4 # poner mas\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0367,  0.0072]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en el dataloader hay alguna manera de añadirle un conjunto de validacion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5031e-06, 5.2914e-06],\n",
       "        [5.8791e-06, 5.0125e-06],\n",
       "        [6.2946e-06, 4.1114e-06],\n",
       "        [1.5288e-05, 6.1703e-06],\n",
       "        [6.3201e-05, 7.6848e-06],\n",
       "        [2.8571e+00, 7.7249e-06],\n",
       "        [2.4096e+00, 8.3817e-06],\n",
       "        [2.0833e+00, 1.4402e-05],\n",
       "        [1.8349e+00, 1.8074e+00],\n",
       "        [1.6393e+00, 2.2303e+00],\n",
       "        [6.9978e-05, 3.3749e+00],\n",
       "        [1.2749e-05, 3.7000e+00],\n",
       "        [6.8515e-06, 4.0250e+00],\n",
       "        [1.1909e-05, 1.8498e-05],\n",
       "        [2.2888e-04, 1.3794e-05],\n",
       "        [9.9999e-01, 1.9565e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2080, 0.6034]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2080, 0.6034]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = model(torch.tensor([[0.25]]))\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64),\n",
       " tensor([[0.2080, 0.6034]], grad_fn=<StackBackward0>),\n",
       " tensor(0.6383, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para mañana revisar la funcion de smooth_lp y la funcion de perdida y ver si puedo sustituir la\n",
    "# funcion smooth_lp por la funcion de linprog de deep_inv_opt. Tambien organizar y explicar un poco\n",
    "# mejor el codigo y ver por qué el loss no esta disminuyendo al entrenar el modelo. Entrenar el modelo\n",
    "# con mas epocas y ver si mejora."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
