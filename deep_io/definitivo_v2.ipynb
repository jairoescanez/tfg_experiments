{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: obtenemos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import deep_inv_opt as io\n",
    "import deep_inv_opt.plot as iop\n",
    "import torch\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 0  # Let the plots flow!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.5484],\n",
       "        [ 0.5968],\n",
       "        ...,\n",
       "        [49.9032],\n",
       "        [49.9516],\n",
       "        [50.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train = io.tensor(np.linspace(0.5, 50, 1024).reshape((-1, 1)))\n",
    "u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000],\n",
       "        [ 0.6941],\n",
       "        [ 0.8882],\n",
       "        [ 1.0824],\n",
       "        [ 1.2765],\n",
       "        [ 1.4706],\n",
       "        [ 1.6647],\n",
       "        [ 1.8588],\n",
       "        [ 2.0529],\n",
       "        [ 2.2471],\n",
       "        [ 2.4412],\n",
       "        [ 2.6353],\n",
       "        [ 2.8294],\n",
       "        [ 3.0235],\n",
       "        [ 3.2176],\n",
       "        [ 3.4118],\n",
       "        [ 3.6059],\n",
       "        [ 3.8000],\n",
       "        [ 3.9941],\n",
       "        [ 4.1882],\n",
       "        [ 4.3824],\n",
       "        [ 4.5765],\n",
       "        [ 4.7706],\n",
       "        [ 4.9647],\n",
       "        [ 5.1588],\n",
       "        [ 5.3529],\n",
       "        [ 5.5471],\n",
       "        [ 5.7412],\n",
       "        [ 5.9353],\n",
       "        [ 6.1294],\n",
       "        [ 6.3235],\n",
       "        [ 6.5176],\n",
       "        [ 6.7118],\n",
       "        [ 6.9059],\n",
       "        [ 7.1000],\n",
       "        [ 7.2941],\n",
       "        [ 7.4882],\n",
       "        [ 7.6824],\n",
       "        [ 7.8765],\n",
       "        [ 8.0706],\n",
       "        [ 8.2647],\n",
       "        [ 8.4588],\n",
       "        [ 8.6529],\n",
       "        [ 8.8471],\n",
       "        [ 9.0412],\n",
       "        [ 9.2353],\n",
       "        [ 9.4294],\n",
       "        [ 9.6235],\n",
       "        [ 9.8176],\n",
       "        [10.0118],\n",
       "        [10.2059],\n",
       "        [10.4000],\n",
       "        [10.5941],\n",
       "        [10.7882],\n",
       "        [10.9824],\n",
       "        [11.1765],\n",
       "        [11.3706],\n",
       "        [11.5647],\n",
       "        [11.7588],\n",
       "        [11.9529],\n",
       "        [12.1471],\n",
       "        [12.3412],\n",
       "        [12.5353],\n",
       "        [12.7294],\n",
       "        [12.9235],\n",
       "        [13.1176],\n",
       "        [13.3118],\n",
       "        [13.5059],\n",
       "        [13.7000],\n",
       "        [13.8941],\n",
       "        [14.0882],\n",
       "        [14.2824],\n",
       "        [14.4765],\n",
       "        [14.6706],\n",
       "        [14.8647],\n",
       "        [15.0588],\n",
       "        [15.2529],\n",
       "        [15.4471],\n",
       "        [15.6412],\n",
       "        [15.8353],\n",
       "        [16.0294],\n",
       "        [16.2235],\n",
       "        [16.4176],\n",
       "        [16.6118],\n",
       "        [16.8059],\n",
       "        [17.0000],\n",
       "        [17.1941],\n",
       "        [17.3882],\n",
       "        [17.5824],\n",
       "        [17.7765],\n",
       "        [17.9706],\n",
       "        [18.1647],\n",
       "        [18.3588],\n",
       "        [18.5529],\n",
       "        [18.7471],\n",
       "        [18.9412],\n",
       "        [19.1353],\n",
       "        [19.3294],\n",
       "        [19.5235],\n",
       "        [19.7176],\n",
       "        [19.9118],\n",
       "        [20.1059],\n",
       "        [20.3000],\n",
       "        [20.4941],\n",
       "        [20.6882],\n",
       "        [20.8824],\n",
       "        [21.0765],\n",
       "        [21.2706],\n",
       "        [21.4647],\n",
       "        [21.6588],\n",
       "        [21.8529],\n",
       "        [22.0471],\n",
       "        [22.2412],\n",
       "        [22.4353],\n",
       "        [22.6294],\n",
       "        [22.8235],\n",
       "        [23.0176],\n",
       "        [23.2118],\n",
       "        [23.4059],\n",
       "        [23.6000],\n",
       "        [23.7941],\n",
       "        [23.9882],\n",
       "        [24.1824],\n",
       "        [24.3765],\n",
       "        [24.5706],\n",
       "        [24.7647],\n",
       "        [24.9588],\n",
       "        [25.1529],\n",
       "        [25.3471],\n",
       "        [25.5412],\n",
       "        [25.7353],\n",
       "        [25.9294],\n",
       "        [26.1235],\n",
       "        [26.3176],\n",
       "        [26.5118],\n",
       "        [26.7059],\n",
       "        [26.9000],\n",
       "        [27.0941],\n",
       "        [27.2882],\n",
       "        [27.4824],\n",
       "        [27.6765],\n",
       "        [27.8706],\n",
       "        [28.0647],\n",
       "        [28.2588],\n",
       "        [28.4529],\n",
       "        [28.6471],\n",
       "        [28.8412],\n",
       "        [29.0353],\n",
       "        [29.2294],\n",
       "        [29.4235],\n",
       "        [29.6176],\n",
       "        [29.8118],\n",
       "        [30.0059],\n",
       "        [30.2000],\n",
       "        [30.3941],\n",
       "        [30.5882],\n",
       "        [30.7824],\n",
       "        [30.9765],\n",
       "        [31.1706],\n",
       "        [31.3647],\n",
       "        [31.5588],\n",
       "        [31.7529],\n",
       "        [31.9471],\n",
       "        [32.1412],\n",
       "        [32.3353],\n",
       "        [32.5294],\n",
       "        [32.7235],\n",
       "        [32.9176],\n",
       "        [33.1118],\n",
       "        [33.3059],\n",
       "        [33.5000],\n",
       "        [33.6941],\n",
       "        [33.8882],\n",
       "        [34.0824],\n",
       "        [34.2765],\n",
       "        [34.4706],\n",
       "        [34.6647],\n",
       "        [34.8588],\n",
       "        [35.0529],\n",
       "        [35.2471],\n",
       "        [35.4412],\n",
       "        [35.6353],\n",
       "        [35.8294],\n",
       "        [36.0235],\n",
       "        [36.2176],\n",
       "        [36.4118],\n",
       "        [36.6059],\n",
       "        [36.8000],\n",
       "        [36.9941],\n",
       "        [37.1882],\n",
       "        [37.3824],\n",
       "        [37.5765],\n",
       "        [37.7706],\n",
       "        [37.9647],\n",
       "        [38.1588],\n",
       "        [38.3529],\n",
       "        [38.5471],\n",
       "        [38.7412],\n",
       "        [38.9353],\n",
       "        [39.1294],\n",
       "        [39.3235],\n",
       "        [39.5176],\n",
       "        [39.7118],\n",
       "        [39.9059],\n",
       "        [40.1000],\n",
       "        [40.2941],\n",
       "        [40.4882],\n",
       "        [40.6824],\n",
       "        [40.8765],\n",
       "        [41.0706],\n",
       "        [41.2647],\n",
       "        [41.4588],\n",
       "        [41.6529],\n",
       "        [41.8471],\n",
       "        [42.0412],\n",
       "        [42.2353],\n",
       "        [42.4294],\n",
       "        [42.6235],\n",
       "        [42.8176],\n",
       "        [43.0118],\n",
       "        [43.2059],\n",
       "        [43.4000],\n",
       "        [43.5941],\n",
       "        [43.7882],\n",
       "        [43.9824],\n",
       "        [44.1765],\n",
       "        [44.3706],\n",
       "        [44.5647],\n",
       "        [44.7588],\n",
       "        [44.9529],\n",
       "        [45.1471],\n",
       "        [45.3412],\n",
       "        [45.5353],\n",
       "        [45.7294],\n",
       "        [45.9235],\n",
       "        [46.1176],\n",
       "        [46.3118],\n",
       "        [46.5059],\n",
       "        [46.7000],\n",
       "        [46.8941],\n",
       "        [47.0882],\n",
       "        [47.2824],\n",
       "        [47.4765],\n",
       "        [47.6706],\n",
       "        [47.8647],\n",
       "        [48.0588],\n",
       "        [48.2529],\n",
       "        [48.4471],\n",
       "        [48.6412],\n",
       "        [48.8353],\n",
       "        [49.0294],\n",
       "        [49.2235],\n",
       "        [49.4176],\n",
       "        [49.6118],\n",
       "        [49.8059],\n",
       "        [50.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_val = io.tensor(np.linspace(0.5, 50, 256).reshape((-1, 1)))\n",
    "u_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora generamos los x correspondientes del modelo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExamplePLP(io.ParametricLP):\n",
    "    # Generate an LP from a given feature vector u and weight vector w.\n",
    "    def generate(self, u, w):\n",
    "        c = [[torch.cos(w + u**2 / 2)],\n",
    "             [torch.sin(w + u**2 / 2)]]\n",
    "\n",
    "        A_ub = [[-1.0,  0.0],      # x1 >= 0\n",
    "                [ 0.0, -1.0],      # x2 >= 0\n",
    "                [ 1.0,  0.0],      # x1 <= 2*w\n",
    "                [ .5*w, w]]  # (1+w)*x1 + 2*(1+w)*x2 <= u\n",
    "\n",
    "        b_ub = [[ 0.0],\n",
    "                [ 0.0],\n",
    "                [ 4/u],\n",
    "                [   u]]\n",
    "        \n",
    "        return c, A_ub, b_ub, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "plp_true = ExamplePLP(weights=[0.8])\n",
    "\n",
    "# Generate training targets by solve the true PLP at each u value.\n",
    "# x_train = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "# torch.save(x_train, \"x_train.pt\")\n",
    "\n",
    "# x_val = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_train])\n",
    "# torch.save(x_val, \"x_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3378e-06, 4.7761e-06],\n",
       "        [6.4988e-06, 4.6438e-06],\n",
       "        [6.1619e-06, 4.1501e-06],\n",
       "        ...,\n",
       "        [8.0112e-02, 1.4527e-05],\n",
       "        [1.9709e-04, 6.2439e+01],\n",
       "        [2.5363e-05, 5.3045e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = torch.load(\"x_val.pt\")\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.3378e-06, 4.7761e-06],\n",
       "        [6.4988e-06, 4.6438e-06],\n",
       "        [6.1619e-06, 4.1501e-06],\n",
       "        ...,\n",
       "        [8.0112e-02, 1.4527e-05],\n",
       "        [1.9709e-04, 6.2439e+01],\n",
       "        [2.5363e-05, 5.3045e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = torch.load(\"x_train.pt\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2: definimos la red y la entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# ver si estoy usando GPU\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "epochs = 100\n",
    "tol=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos el dataset\n",
    "class UDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data.clone().to(dtype=torch.float32) # nota: esta dando el warning porque estoy convirtiendo un tensor a otro, en ese caso es mejor usar clone()\n",
    "        # si los datos de entrada no los voy a dar como un tensor, entonces hay que poner lo que he puesto: self.data = torch.tensor(data, dtype=torch.float32), self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        self.targets = targets.clone().to(dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "# Dataset con pares (u, x)\n",
    "dataset = UDataset(u_train, x_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = UDataset(u_val, x_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion que resuelve el problema de programacion lineal (por ahora nos la creemos, pero hay que revisarla)\n",
    "# hay que tener en cuenta que esta funcion debe realizarse con operaciones de pytorch para\n",
    "# preservar el grafo de computo para poder hacer backpropagation\n",
    "# def smooth_lp(c, A, b):\n",
    "#     # Inicializar x con gradientes habilitados\n",
    "#     x = torch.zeros(A.shape[1], requires_grad=True)\n",
    "#     optimizer = torch.optim.SGD([x], lr=1e-3)\n",
    "\n",
    "#     for _ in range(1000):\n",
    "#         optimizer.zero_grad()\n",
    "#         constraint_penalty = torch.sum(torch.relu(A @ x - b))\n",
    "#         objective = torch.dot(c, x) + 100.0 * constraint_penalty\n",
    "#         objective.backward(retain_graph=True)  # Mantén el grafo activo\n",
    "#         optimizer.step()\n",
    "#     return x  # Sin detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voy a definir otra funcion para resolver el problema de programacion lineal, pero esta vez con el solver de pytorch\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def smooth_lp(c, A, b):\n",
    "    c = c.detach().cpu().numpy()\n",
    "    A = A.detach().cpu().numpy()\n",
    "    b = b.detach().cpu().numpy()\n",
    "\n",
    "    res = opt.linprog(c, A_ub=A, b_ub=b, method='highs-ipm')\n",
    "    print(res)\n",
    "    return torch.tensor(res.x, dtype=torch.float32)\n",
    "\n",
    "# def smooth_lp(c, A, b):\n",
    "#     return torch.tensor(opt.linprog(c, A_ub=A, b_ub=b, method='highs-ipm').x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([1.0, 1.0])\n",
    "A = torch.tensor([[-1.0, 0.0],\n",
    "                  [0.0, -1.0]])\n",
    "b = torch.tensor([0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0.])"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_lp(c, A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta version es la red neuronal en la que la resolucion del problema de optimizacion esta dentro de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la red (hay que revisar la forma de la red y el por qué)\n",
    "class ParametricLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParametricLPNet, self).__init__()\n",
    "        # Entrada de dimensión 1, salida 8 (2 para c, 4 para A, 2 para b)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)  # c (2), A (4), b (2)\n",
    "        )\n",
    "\n",
    "    def forward(self, u):\n",
    "        output = self.fc(u)\n",
    "        c = output[:, 0:2]      # Vector de costes\n",
    "        A = output[:, 2:6].reshape(-1, 2, 2)  # Matriz A (2x2)\n",
    "        b = output[:, 6:8]      # Vector de restricciones\n",
    "        return torch.stack([smooth_lp(c[i], A[i], b[i]) for i in range(u.shape[0])]) # Resolver LPs ver si hay alguna manera mas eficiente de hacer esto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de perdida\n",
    "# def loss_fn(rs, target):\n",
    "#     '''\n",
    "#     rs: lista de tensores que son las salidas de la red\n",
    "#     target: lista de tensores que son las salidas deseadas\n",
    "\n",
    "#     Esta función calcula la suma de las distancias al cuadrado entre las salidas de la red y las salidas deseadas.\n",
    "#     '''\n",
    "#     return torch.sum(torch.linalg.vector_norm(rs-target, ord=2, dim=1).pow(2))\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear la red neuronal\n",
    "model = ParametricLPNet()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # elegir una de las dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# para ver el modelo\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicializamod los pesos de la red \n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu') # inicializamos los pesos de la red con Kaiming\n",
    "        nn.init.zeros_(m.bias) # inicializamos los bias a 0\n",
    "\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def near(rs, x_batch, tol=1e-3):\n",
    "    '''\n",
    "    rs: tensor formado por las soluciones de los LPs\n",
    "    x_batch: tensor formado por las soluciones deseadas de los LPs\n",
    "    tol: tolerancia para considerar que dos vectores son iguales\n",
    "\n",
    "    Esta función calcula el número de soluciones de los LPs que están \n",
    "    a una distancia menor que tol de las soluciones deseadas.\n",
    "    '''\n",
    "    matriz_diferencias = rs - x_batch # matriz de diferencias entre las soluciones de los LPs y las soluciones deseadas\n",
    "    matriz_al_cuadrado = matriz_diferencias ** 2 # elevo cada elemento al cuadrado\n",
    "    distancias = torch.sum(matriz_al_cuadrado, dim=1) # sumo los elementos de cada fila\n",
    "    return torch.sum(distancias < tol).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss_media = 0.0\n",
    "    accuracy = 0.0\n",
    "    batch_size = dataloader.batch_size\n",
    "    for batch, (u_batch, x_batch) in enumerate(dataloader):\n",
    "\n",
    "        rs = model(u_batch)\n",
    "        loss = loss_fn(rs, x_batch) # Calcular la pérdida\n",
    "\n",
    "        # Backpropagation y optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 8 == 0: # cambiar este numero para que salga cada cierto numero de iteraciones\n",
    "            loss = loss.item()\n",
    "            current = batch * batch_size + batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        loss_media += loss\n",
    "        accuracy += near(rs, x_batch, tol)\n",
    "\n",
    "    return loss_media / num_batches, accuracy / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size=len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    #with torch.no_grad():\n",
    "    for u_batch, x_batch in dataloader:\n",
    "        rs = model(u_batch)\n",
    "        test_loss += loss_fn(rs, x_batch).item()\n",
    "        correct += near(rs, x_batch, tol) # consideramos correcto si se acerca a la solucion en la distancia euclidea \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "        message: Optimization terminated successfully. (HiGHS Status 7: Optimal)\n",
      "        success: True\n",
      "         status: 0\n",
      "            fun: -0.02193313677876964\n",
      "              x: [ 0.000e+00  1.830e-01]\n",
      "            nit: 6\n",
      "          lower:  residual: [ 0.000e+00  1.830e-01]\n",
      "                 marginals: [ 1.345e-02  0.000e+00]\n",
      "          upper:  residual: [       inf        inf]\n",
      "                 marginals: [ 0.000e+00  0.000e+00]\n",
      "          eqlin:  residual: []\n",
      "                 marginals: []\n",
      "        ineqlin:  residual: [ 0.000e+00  7.405e-02]\n",
      "                 marginals: [-3.102e-01  0.000e+00]\n",
      " mip_node_count: 0\n",
      " mip_dual_bound: 0.0\n",
      "        mip_gap: 0.0\n",
      "       message: The problem is unbounded. (HiGHS Status 10: model_status is Unbounded; primal_status is At upper bound)\n",
      "       success: False\n",
      "        status: 3\n",
      "           fun: None\n",
      "             x: None\n",
      "           nit: 1\n",
      "         lower:  residual: None\n",
      "                marginals: None\n",
      "         upper:  residual: None\n",
      "                marginals: None\n",
      "         eqlin:  residual: None\n",
      "                marginals: None\n",
      "       ineqlin:  residual: None\n",
      "                marginals: None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[452], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Error: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39maccuracy)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>0.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%, Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     test_loop(val_dataloader, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[450], line 10\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (u_batch, x_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m---> 10\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(rs, x_batch) \u001b[38;5;66;03m# Calcular la pérdida\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Backpropagation y optimización\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JairoEscanez\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\JairoEscanez\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[444], line 19\u001b[0m, in \u001b[0;36mParametricLPNet.forward\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m     17\u001b[0m A \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Matriz A (2x2)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m6\u001b[39m:\u001b[38;5;241m8\u001b[39m]      \u001b[38;5;66;03m# Vector de restricciones\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43msmooth_lp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(u\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])])\n",
      "Cell \u001b[1;32mIn[441], line 11\u001b[0m, in \u001b[0;36msmooth_lp\u001b[1;34m(c, A, b)\u001b[0m\n\u001b[0;32m      9\u001b[0m res \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mlinprog(c, A_ub\u001b[38;5;241m=\u001b[39mA, b_ub\u001b[38;5;241m=\u001b[39mb, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhighs-ipm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 4 # poner mas\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    loss, accuracy = train_loop(dataloader, model, loss_fn, optimizer)\n",
    "    print(f\"\\nTraining Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    \n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3920, -2.2971]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000],\n",
       "        [0.3600],\n",
       "        [0.6200],\n",
       "        [0.8800],\n",
       "        [1.1400],\n",
       "        [1.4000],\n",
       "        [1.6600],\n",
       "        [1.9200],\n",
       "        [2.1800],\n",
       "        [2.4400],\n",
       "        [2.7000],\n",
       "        [2.9600],\n",
       "        [3.2200],\n",
       "        [3.4800],\n",
       "        [3.7400],\n",
       "        [4.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_test = torch.tensor(np.linspace(0.1, 4, 16).reshape((-1, 1)), dtype=torch.float64)\n",
    "u_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora probamos como funciona para un x nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5031e-06, 5.2914e-06],\n",
       "        [5.8791e-06, 5.0125e-06],\n",
       "        [6.2946e-06, 4.1114e-06],\n",
       "        [1.5288e-05, 6.1703e-06],\n",
       "        [6.3201e-05, 7.6848e-06],\n",
       "        [2.8571e+00, 7.7249e-06],\n",
       "        [2.4096e+00, 8.3817e-06],\n",
       "        [2.0833e+00, 1.4402e-05],\n",
       "        [1.8349e+00, 1.8074e+00],\n",
       "        [1.6393e+00, 2.2303e+00],\n",
       "        [6.9978e-05, 3.3749e+00],\n",
       "        [1.2749e-05, 3.7000e+00],\n",
       "        [6.8515e-06, 4.0250e+00],\n",
       "        [1.1909e-05, 1.8498e-05],\n",
       "        [2.2888e-04, 1.3794e-05],\n",
       "        [9.9999e-01, 1.9565e-05]], dtype=torch.float64)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u_test])\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParametricLPNet(\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=8, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2500]], dtype=torch.float64),\n",
       " tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64))"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([[0.25]], dtype=torch.float64)\n",
    "x = torch.cat([io.linprog(*plp_true(ui)).detach().t() for ui in u])\n",
    "u,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9870, -0.9380]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([[0.25]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9870, -0.9380]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred = model(torch.tensor([[0.25]]))\n",
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.6596e-06, 5.1630e-06]], dtype=torch.float64),\n",
       " tensor([[-0.9870, -0.9380]], grad_fn=<StackBackward0>),\n",
       " tensor(1.3617, dtype=torch.float64, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x_pred, torch.norm(x - x_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar:\n",
    "- Función de pérdida ✔️\n",
    "- Función smooth_lp (a ver si puedo sustituir la funcion smooth_lp por la funcion de linprog de deep_inv_opt)\n",
    "- Organizar y explicar el código\n",
    "- Estudiar por qué no disminuye el loss cuando entreno el modelo: es porque está mostrando el loss en el conjunto de test y no en el de entrenamiento ✔️\n",
    "- Entrenar el modelo con más épocas y datos\n",
    "- Pensar qué optimizador es mejor ✔️\n",
    "- Pensar qué estructura de la red es mejor ✔️\n",
    "- Inicialización de los pesos de la red ✔️\n",
    "- Arreglar el porcentaje de aciertos ✔️\n",
    "\n",
    "\n",
    "\n",
    "Bitácora: ahora lo que pasa es que da error porque el problema de optimización no está acotado, lo que tengo que hacer es revisar en el paper el intervalo donde se mueve la u porque creo que lo he puesto mal. Cuando arregle esto, tengo que volver a entrenar el modelo y ver si se está entrenando bien. También tengo que probar resolviendo el problema de optimización con la función de linprog de deep_inv_opt y con la de cvxpy a ver si alguna de las dos funciona mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
